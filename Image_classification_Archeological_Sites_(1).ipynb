{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksPTAlQfwEKs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import InceptionV3, EfficientNetB0, ResNet50, VGG16\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_size = (224, 224)\n",
        "SEED = 42\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1024\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "metadata": {
        "id": "WN8qcgDI8JEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ImageProcessor:"
      ],
      "metadata": {
        "id": "vQe05trv4EMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageProcessor:\n",
        "\n",
        "    @staticmethod\n",
        "    def augment_images(images):\n",
        "        datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        augmented_images = []\n",
        "        for img in images:\n",
        "            img = img.reshape((1,) + img.shape)\n",
        "            for batch in datagen.flow(img, batch_size=1):\n",
        "                augmented_images.append(batch[0])\n",
        "                break\n",
        "\n",
        "        return np.array(augmented_images)"
      ],
      "metadata": {
        "id": "FOTFiKP22vth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DataLoader"
      ],
      "metadata": {
        "id": "zpj98j5IKG11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "\n",
        "    @staticmethod\n",
        "    def read_and_resize_images(folder_path, label, target_shape=(224, 224)):\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                img_path = os.path.join(folder_path, filename)\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    img = cv2.resize(img, target_shape)\n",
        "                    if img.shape[:2] == target_shape:\n",
        "                        images.append(img)\n",
        "                        labels.append(label)\n",
        "\n",
        "        num_images = len(images)\n",
        "        print(f\"Loaded {num_images} images for class {label} from folder {folder_path}\")\n",
        "        return images, labels\n",
        "\n",
        "    @staticmethod\n",
        "    def load_dataset(root_folder):\n",
        "        all_images = []\n",
        "        all_labels = []\n",
        "        class_folders = sorted(os.listdir(root_folder))\n",
        "\n",
        "        for label, class_folder in enumerate(class_folders):\n",
        "            class_path = os.path.join(root_folder, class_folder)\n",
        "            if os.path.isdir(class_path):\n",
        "                images, labels = DataLoader.read_and_resize_images(class_path, label)\n",
        "                all_images.extend(images)\n",
        "                all_labels.extend(labels)\n",
        "\n",
        "        return class_folders, np.array(all_images), np.array(all_labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_images(images, labels, class_folders):\n",
        "        num_classes = len(class_folders)\n",
        "        fig, axes = plt.subplots(1, num_classes, figsize=(15, 3))\n",
        "\n",
        "        for i in range(num_classes):\n",
        "            class_images = images[labels == i]\n",
        "            axes[i].imshow(class_images[0][:, :, ::-1])\n",
        "            axes[i].set_title(f\" {class_folders[i]}\")\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def split_dataset(images, labels, batch_size, seed=42, validation_size=0.2):\n",
        "        # Split dataset into training and validation sets\n",
        "        train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "            images, labels, test_size=validation_size, random_state=seed)\n",
        "\n",
        "        # Create tf.data.Dataset for training and validation\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "\n",
        "        # Shuffle and batch the training dataset, and prefetch it for better performance\n",
        "        train_dataset = train_dataset.shuffle(buffer_size=len(train_images), seed=seed).batch(batch_size).prefetch(AUTOTUNE)\n",
        "\n",
        "        # Batch the validation dataset, and prefetch it\n",
        "        val_dataset = val_dataset.batch(batch_size).prefetch(AUTOTUNE)\n",
        "\n",
        "        return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "G6UND1jew_6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "\n",
        "    @staticmethod\n",
        "    def build_model(base_model, num_classes):\n",
        "        # Freeze the base model layers\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Create a custom model on top of the pre-trained base\n",
        "        model = models.Sequential()\n",
        "        model.add(base_model)\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(256, activation='relu'))\n",
        "        model.add(layers.Dropout(0.5))\n",
        "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def compile_model(model):\n",
        "        # Compile the model\n",
        "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def train_model(model, train_dataset, val_dataset, epochs=5):\n",
        "        # Train the model using tf.data.Dataset\n",
        "        history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset)\n",
        "        return model, history\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_history(history):\n",
        "        # Plotting losses and accuracy\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Plot training & validation accuracy values\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title('Model accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "        # Plot training & validation loss values\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Model loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(model, file_path='saved_model.h5'):\n",
        "        # Save the model\n",
        "        model.save(file_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model(file_path='saved_model.h5'):\n",
        "        # Load the saved model\n",
        "        loaded_model = tf.keras.models.load_model(file_path)\n",
        "        return loaded_model\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_model(loaded_model, val_dataset):\n",
        "        # Evaluate the loaded model\n",
        "        eval_result = loaded_model.evaluate(val_dataset)\n",
        "        print(\"Loaded Model - Loss: {}, Accuracy: {}\".format(eval_result[0], eval_result[1]))\n",
        "\n",
        "    @staticmethod\n",
        "    def predict_with_model(loaded_model, sample_image_path, target_size):\n",
        "        # Make predictions using the loaded model\n",
        "        img = tf.keras.preprocessing.image.load_img(sample_image_path, target_size=target_size)\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = tf.expand_dims(img_array, 0)\n",
        "        img_array /= 255.0\n",
        "\n",
        "        predictions = loaded_model.predict(img_array)\n",
        "        predicted_class = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        print(\"Predicted Class:\", predicted_class)\n"
      ],
      "metadata": {
        "id": "P7oVkEfmxGEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Application:\n",
        "\n",
        "    @staticmethod\n",
        "    def run(root_folder='/content/drive/MyDrive/ Archaeological Sites in Jordan (1)', target_size=(224, 224),\n",
        "            batch_size=32, epochs=5):\n",
        "        # Step 1: Load and preprocess data\n",
        "        data_loader = DataLoader()\n",
        "        class_folders, images, labels = data_loader.load_dataset(root_folder)\n",
        "        data_loader.plot_images(images, labels, class_folders)\n",
        "\n",
        "        # Step 2: Augment images\n",
        "        augmented_images = ImageProcessor.augment_images(images)\n",
        "        images = np.concatenate((images, augmented_images), axis=0)\n",
        "        labels = np.concatenate((labels, labels), axis=0)\n",
        "\n",
        "        # Step 3: Split dataset into training, validation\n",
        "        train_dataset, val_dataset = DataLoader.split_dataset(images, labels, BATCH_SIZE, seed=SEED)\n",
        "\n",
        "        # Step 4: Model training using InceptionV3\n",
        "        inception_base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "        custom_model = ModelTrainer.build_model(inception_base_model, len(class_folders))\n",
        "        compiled_model = ModelTrainer.compile_model(custom_model)\n",
        "        trained_model, history = ModelTrainer.train_model(compiled_model, train_dataset, val_dataset, epochs=epochs)\n",
        "        ModelTrainer.plot_history(history)\n",
        "        ModelTrainer.save_model(trained_model, 'inception_model.h5')\n",
        "\n",
        "        # Step 5: Model training using EfficientNet\n",
        "        efficientnet_base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "        custom_model = ModelTrainer.build_model(efficientnet_base_model, len(class_folders))\n",
        "        compiled_model = ModelTrainer.compile_model(custom_model)\n",
        "        trained_model, history = ModelTrainer.train_model(compiled_model, train_dataset, val_dataset, epochs=epochs)\n",
        "        ModelTrainer.plot_history(history)\n",
        "        ModelTrainer.save_model(trained_model, 'efficientnet_model.h5')\n",
        "\n",
        "        # Step 6: Model training using ResNet50\n",
        "        resnet_base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "        custom_model = ModelTrainer.build_model(resnet_base_model, len(class_folders))\n",
        "        compiled_model = ModelTrainer.compile_model(custom_model)\n",
        "        trained_model, history = ModelTrainer.train_model(compiled_model, train_dataset, val_dataset, epochs=epochs)\n",
        "        ModelTrainer.plot_history(history)\n",
        "        ModelTrainer.save_model(trained_model, 'resnet_model.h5')\n",
        "\n",
        "        # Step 7: Model training using VGG16\n",
        "        vgg_base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "        custom_model = ModelTrainer.build_model(vgg_base_model, len(class_folders))\n",
        "        compiled_model = ModelTrainer.compile_model(custom_model)\n",
        "        trained_model, history = ModelTrainer.train_model(compiled_model, train_dataset, val_dataset, epochs=epochs)\n",
        "        ModelTrainer.plot_history(history)\n",
        "        ModelTrainer.save_model(trained_model, 'vgg_model.h5')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Application.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "DiXo_TcVxM31",
        "outputId": "24fd6af4-a9b1-47eb-a4b2-e73acdb164c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 0 images for class 0 from folder /content/drive/MyDrive/ Archaeological Sites in Jordan (1)/test\n",
            "Loaded 0 images for class 1 from folder /content/drive/MyDrive/ Archaeological Sites in Jordan (1)/train\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 0 is out of bounds for axis 0 with size 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fccdbf4c3d1f>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mApplication\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-fccdbf4c3d1f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(root_folder, target_size, batch_size, epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mclass_folders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_folders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Step 2: Augment images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2a46b4d02353>\u001b[0m in \u001b[0;36mplot_images\u001b[0;34m(images, labels, class_folders)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mclass_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" {class_folders[i]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}